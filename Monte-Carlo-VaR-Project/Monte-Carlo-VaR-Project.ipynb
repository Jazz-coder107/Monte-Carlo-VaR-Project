{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YweISEs7E92O"
   },
   "source": [
    "\n",
    "#### Date created: 16/06/2025\n",
    "#### Completed by Jasveer Govender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wv1Yjp2dHMfZ"
   },
   "source": [
    "# **Topic: Monte-Carlo Simulations to compute VaR and Expected Shortfall**\n",
    "<div class=\"alert alert-info \">\n",
    "    <strong>Problem statement:</strong>\n",
    "\n",
    "In simple terms: *We’re trying to answer the question: \"If markets behave randomly based on historical returns, how bad could my losses get?\"*\n",
    "\n",
    "In this project, you will compute the following using Python:\n",
    "- Historical VaR and CVaR\n",
    "- Parametric VaR and CVaR\n",
    "- Monte Carlo VaR and CVaR\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SoOqsQ7FgmF"
   },
   "source": [
    "<div class=\"alert alert-info \">\n",
    "    <strong>Steps to perform for this project:</strong>\n",
    "\n",
    "1. Import the libraries\n",
    "2. Download the data\n",
    "3. Analyze the returns distribution\n",
    "4. Compute the historical VaR and C-VaR (Expected shortfall)\n",
    "5. Compute the parametric VaR and C-VaR (Expected shortfall)\n",
    "6. Scaling the VaR\n",
    "7. Monte Carlo simulations\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Definitions to know:\n",
    "\n",
    "**Monte Carlo Simulation:** A method to simulate many possible outcomes using random sampling. Think of it like rolling dice many times to see all possible results.\n",
    "\n",
    "**VaR (Value at Risk):** The loss level that is not expected to be exceeded with a certain confidence level. E.g., 95% VaR = “We expect not to lose more than this amount 95% of the time.”\n",
    "\n",
    "**Expected Shortfall (ES):** The average loss that occurs in the worst-case scenarios (those beyond the VaR threshold). It gives a more complete picture of risk than VaR alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeR_LTSEJ55r"
   },
   "source": [
    "### **Section 1: Import the libraries**\n",
    "\n",
    "Import the libraries with the aliases given in the brackets\n",
    " \n",
    "- yfinance (yf)\n",
    "- pandas (pd)\n",
    "- numpy (np)\n",
    "- matplotlib.pyplot (plt)\n",
    "- datetime (dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2aeVpjYiNGU"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6R3l3j61J0K"
   },
   "outputs": [],
   "source": [
    "# Run this before importing yfinance in Google Colab\n",
    "# im using VS Code and already did this\n",
    "#!pip install yfinance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ivlLWoHYFdYw"
   },
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEW3IMhZS6RN"
   },
   "outputs": [],
   "source": [
    "# Settings the figsize parameter for the plots in this notebook to standardize the size of plots\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uag43IdQOm-X"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pO-7wmSKKo-o"
   },
   "source": [
    "### **Section 2: Analyze the returns distribution**\n",
    "\n",
    "**Step 1**\n",
    "\n",
    "In order to calculate returns, you'll first need stock price data. For this, you can fetch data from Yahoo Finance using `yfinance`.\n",
    "\n",
    "Follow these steps:\n",
    "1.   Set the parameters for start and end date, and the ticker.\n",
    "2.   Import the data from Yahoo Finance using the `yfinance` library.\n",
    "3.   Print a message saying '< n > number of records downloaded'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UH_YYxHWOqDP"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Enter the code for step 1 here\n",
    "'''\n",
    "start_date = \"2020-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "ticker = \"NVDA\"\n",
    "\n",
    "data = yf.download(ticker,start_date, end_date)\n",
    "print(f\"{len(data)} records downlaoded for {ticker}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFQeEBTJPYGl"
   },
   "source": [
    "**Step 2**\n",
    "\n",
    "Now that you've downloaded the data, you need to compute the simple daily returns.\n",
    "\n",
    "Follow these steps:\n",
    "1. Compute simple returns\n",
    "2. Check the first five rows of the dataframe.\n",
    "3. Check the last five rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSWDLj8GRdpM"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Enter the code for step 2 here\n",
    "'''\n",
    "data1 = data.droplevel(1,axis=1)\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .pct_change() calculates the percentage change between the current and a prior element = Simple Return Formula\n",
    "data1['simple_return'] = data1['Close'].pct_change()\n",
    "print(data1.head())\n",
    "print(data1.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDvzD6o2ReSL"
   },
   "source": [
    "**HINT**:\n",
    "You need to call the dot pct_change() method on the proper column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSyUhIkLSMdg"
   },
   "source": [
    "**Step 3**\n",
    "\n",
    "Let's now visualize the return distribution. For this, you will use the `hist` function from the `matplotlib.pyplot` module.\n",
    "However, as we saw earlier, the first row of the returns contains null values. So we need to drop these before passing it to the `hist` function.\n",
    "Setting bins equal to a large number will spread out your plot, but a low number will cause a lack of resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0vchj_qEUd9x"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Enter the code for step 3 here\n",
    "'''\n",
    "plt.hist(data1['simple_return'].dropna(), bins = 100, edgecolor = 'blue')\n",
    "plt.title(\"Distribution of Simple Daily Returns for NVDA\")\n",
    "plt.xlabel(\"Daily Returns\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Histogram above we notice that it is Normaly distributed (bell-shaped) with some extreme values on the tails - possible due to high-impact events (like earnings shocks or flash crashes).\n",
    "\n",
    "Something to note, using **VaR** alone can be misleading - beacuse it assumes normality and underestimates tail risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWRCSed6ohEE"
   },
   "source": [
    "**Step 4**\n",
    "\n",
    "Now, you need to compute the mean and standard deviation of the returns. You also need to compute the annualized average returns using the formula below:\n",
    "\n",
    "$\\text{Average Annualized Return} = ( ( 1 + \\mu ) ^ {252}) - 1$\n",
    "\n",
    "\n",
    "Standard deviation for T time periods can be computed using the following formula:\n",
    "\n",
    "$\\sigma_{annual} = \\sigma_{daily} * \\sqrt{T}$\n",
    "\n",
    "Follow these steps:\n",
    "1. Compute the average daily returns and the annualized returns.\n",
    "2. Compute the standard deviation of the returns and the annualized volatility.\n",
    "3. Compute the annualized variance.\n",
    "4. Compute the skewness and kurtosis of the returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCtLdK-hpioy"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Enter the code for step 4 here\n",
    "'''\n",
    "mu = data1['simple_return'].mean()\n",
    "sigma = data1['simple_return'].std()\n",
    "print(f\"Mean of Simple Returns: {mu}\")\n",
    "print(f\"Standard Deviation of Simple Returns: {sigma}\")\n",
    "print(f\"Annualized Mean of Simple Returns: {mu * 252}\")\n",
    "print(f\"Annualized Standard Deviation of Simple Returns: {sigma * np.sqrt(252)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lx9ilPOMpjyZ"
   },
   "source": [
    "**HINT**: \n",
    "- You can use the np.mean() and np.std() functions to compute the mean and standard deviation of the returns.\n",
    "- You can compute the annualized std by multiplying the daily std with the square root of 252.\n",
    "- You can compute the annualized variance by squaring the annualized std."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuAYVmVMyUPd"
   },
   "source": [
    "Compute the skewness and excess kurtosis of returns using the **skew()** and **kurtosis()** functions from the **scipy.stats** library. \n",
    "Note: You need to add the necessary imports here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjRghMIQys4x"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Add the code to compute the skewness and kurtosis of returns here.\n",
    "'''\n",
    "skewness = data1['simple_return'].skew\n",
    "kurtosis = data1['simple_return'].kurtosis()\n",
    "\n",
    "print(f\"Skewness of Simple Returns: {skewness()}\")\n",
    "print(f\"Kurtosis of Simple Returns: {kurtosis}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voYgoS2dzNmU"
   },
   "source": [
    "**HINT**: \n",
    "- This is the syntax: scipy.stats.skew(_Returns column_). Remember to drop the null values.\n",
    "- This is the syntax: scipy.stats.kurtosis(_Returns column_). Remember to drop the null values.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVW6bi9C2Qyy"
   },
   "source": [
    "**Step 5**\n",
    "\n",
    "Check the normality of the stock returns distribution using the **Shapiro-Wilk test**. You can use the `shapiro()` function from the `scipy.stats` library.\n",
    "\n",
    "The function will return two values- the first value is the t-stat of the test, and the second value is the p-value. You can use the p-value to assess the normality of the data. If the p-value is less than or equal to 0.05, you can  reject the null hypothesis of normality and assume that the data are non-normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the Shapiro-Wilk Test?**\n",
    "\n",
    "    A statistical test used to check if your data is normally distributed.\n",
    "\n",
    "    Null Hypothesis (H₀):Data is normally distributed.\n",
    "\n",
    "    If p-value < 0.05, you reject H₀ → Data is not normal.\n",
    "\n",
    "    If p-value ≥ 0.05 → fail to reject H₀ → Data could be normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHdRYDX13F4b"
   },
   "outputs": [],
   "source": [
    "# Import the shapiro function from the scipy.stats library\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Compute the p_value by running the shapiro function on the returns column\n",
    "stat, p_value = shapiro(data1['simple_return'].dropna())\n",
    "print(f\"Shapiro-Wilk Test Statistic: {stat}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Interpret the results\n",
    "if p_value <= 0.05:\n",
    "    print(\"Null hypothesis of normality is rejected.\")\n",
    "else:\n",
    "    print(\"Null hypothesis of normality is accepted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above result, given the p-value is less than 5% we reject the Null hypothesis as we have enough statistical eveidence to say that our distribution is **not normal** which validates using different methods like *Historical VaR, C-VaR and Monte Carlo.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "se-3pdfiOzXN"
   },
   "source": [
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_QcWSAZPHqx"
   },
   "source": [
    "### **Section 3: Historical VaR and C-VaR (Expected shortfall)**\n",
    "\n",
    "Value at Risk (VaR) is the maximum loss that one will not exceed with a certain probability α within a given time horizon. It is given as a threshold with a given confidence level that losses will not exceed at that level.\n",
    "\n",
    "Conditional Value at Risk (CVaR), or Expected Shortfall, is an estimate of\n",
    "expected losses sustained in the worst (1 - x)% of scenarios.\n",
    "\n",
    "**Step 1**\n",
    "1. Define the parameter for the confidence level for the VaR (say, 95).\n",
    "2. Compute the historical VaR based on the significance level (1 - confidence_level).\n",
    "3. Compute the historical CVaR based on the significance level (1 - confidence_level)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5znWtS4Oz2v"
   },
   "outputs": [],
   "source": [
    "# Define the var level parameter\n",
    "var_level = 95\n",
    "confidence_level =  0.95\n",
    "# Compute and print the historical VaR\n",
    "var_95 = np.percentile(data1['simple_return'].dropna(), (1 - confidence_level) *100)\n",
    "print(f\"Historical VaR (95% Confidence): {var_95: .5f}\")\n",
    "\n",
    "# Sort the returns for plotting\n",
    "sorted_returns = np.sort(data1['simple_return'].dropna())\n",
    "\n",
    "# Plot the probability of each sorted return quantile \n",
    "plt.plot(sorted_returns)\n",
    "plt.title(\"Sorted Daily Returns for NVDA\")\n",
    "plt.xlabel(\"Sorted Returns\")\n",
    "plt.ylabel(\"Returns\")\n",
    "\n",
    "# Draw a vertical line in the plot for the VaR 95 quantile\n",
    "plt.axvline(x=var_95, color='red', linestyle='--', label=f'VaR {var_level}%: {var_95:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRyqHmyXRDqH"
   },
   "source": [
    "**HINT**: You need to use the np.percentile() function. Remember, that you need to compute the lower 5% percentile for VaR(95)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The X-axis shows all returns sorted from worst to best.\n",
    "\n",
    "    The Y-axis is the return value.\n",
    "\n",
    "    The red dashed line at ~ -5.1% shows the VaR at 95% confidence.\n",
    "\n",
    "    Interpretation: 5% of the time, NVDA loses more than 5.1% in a day based on historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVvbvR3BS3ot"
   },
   "source": [
    "**Step 2**\n",
    "\n",
    "Compute the Expected Shortfall (CVaR) and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhaKk2RXS4ct"
   },
   "outputs": [],
   "source": [
    "# Compute and print the expected shortfall\n",
    "# filters all the returns less than or equal to the VaR value.\n",
    "cvar_95 = data1[data1['simple_return'] <= var_95]['simple_return'].mean()\n",
    "print(f\"Expected Shortfall (CvaR 95): {cvar_95: .5f}\")\n",
    "\n",
    "# Sort the returns for plotting\n",
    "#Takes all returns and drops any NaN values before sorting (smallest to largest)\n",
    "sorted_returns = np.sort(data1['simple_return'].dropna())\n",
    "\n",
    "# Plot the probability of each sorted return quantile\n",
    "plt.plot(sorted_returns, label='Sorted Returns')\n",
    "\n",
    "# Draw vertical lines in the plot for the VaR 95 and CVaR quantiles\n",
    "#Adds a red vertical dashed line at the 5th percentile index (VaR)\n",
    "plt.axvline(x=int(0.05 * len(sorted_returns)), color='red', linestyle='--', label=f'VaR 95%: {var_95:.2%}')\n",
    "#Finding where in the sorted_returns list the CVaR value appears\n",
    "#np.where(...) returns the index where it’s equal and then draw a vertical line at that index\n",
    "cvar_index = np.where(sorted_returns == sorted_returns[sorted_returns <= var_95].mean())[0]\n",
    "#Sometimes, the CVaR value doesn’t match exactly any value in the list (due to rounding or float precision). \n",
    "# In that case, we draw a horizontal line instead\n",
    "if len(cvar_index) > 0:\n",
    "    plt.axvline(x=cvar_index[0], color='purple', linestyle='--', label=f'CVaR 95%: {cvar_95:.2%}')\n",
    "else:\n",
    "    # fallback in case no exact match is found\n",
    "    plt.axhline(y=cvar_95, color='purple', linestyle='--', label=f'CVaR 95%: {cvar_95:.2%}')\n",
    "\n",
    "# Finishing touches\n",
    "plt.title(\"Sorted Daily Returns with VaR & CVaR at 95% Confidence\")\n",
    "plt.xlabel(\"Sorted Return Index\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gB3ukighTGUW"
   },
   "source": [
    "**HINT**: For expected shortfall, you need to take a mean of the returns lower than the VaR(95)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a sorted plot of all daily returns for Nvidia over the last few years, from worst to best.\n",
    "\n",
    "The red dashed line marks the Value at Risk (VaR) at a 95% confidence level. What this means is:\n",
    "\n",
    "Based on historical data, we expect that on 95% of trading days, losses won't exceed this level — in this case, around 5.1%.\n",
    "\n",
    "But VaR only tells us where the tail begins, not how bad things can get once we’re in it.\n",
    "\n",
    "So we also calculate the Conditional Value at Risk (CVaR) — shown here as the purple line.\n",
    "\n",
    "This represents the average loss on those worst 5% of days, and in this case, it’s approximately 6.9%.\n",
    "\n",
    "This chart visually demonstrates our tail risk:\n",
    "\n",
    "It quantifies the threshold of concern (VaR), and reveals the expected magnitude of losses beyond that point (CVaR).\n",
    "\n",
    "For risk management and portfolio construction, this is critical — especially in volatile assets like NVDA. It helps us prepare not just for frequent losses, but also for rare but severe ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2gYLg3_jmCd"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXeh5pZ9WBWz"
   },
   "source": [
    "### **Section 4: Parametric VaR and C-VaR (Expected shortfall)**\n",
    "\n",
    "\n",
    "The **parametric method VAR** (also known as **Variance/Covariance VAR**) calculation is another commonly used form of VaR calculation. This method allows you to simulate a range of possibilities based on historical return distribution properties rather than actual return values.\n",
    "\n",
    "\n",
    "You can use the `norm.ppf()` function from the `scipy.stats` library for this. You have already computed the mean and standard deviation of the returns earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametric VaR:\n",
    "VaRα = μ + zα * σ\n",
    "VaRα ​= μ + zα * σ\n",
    "\n",
    "Where:\n",
    "\n",
    "    μ = mean return\n",
    "\n",
    "    σ = std deviation\n",
    "\n",
    "    zα = Z-score from normal distribution (e.g. -1.645 for 5% one-tailed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9lnmwiWkwhu"
   },
   "outputs": [],
   "source": [
    "# Import the necessary library\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Set the confidence level for VaR(95)\n",
    "confidence_level = 0.95\n",
    "alpha =  1 - confidence_level\n",
    "\n",
    "#Also need to calculate the mean and standard deviation of the simple returns\n",
    "mu = data1['simple_return'].mean()\n",
    "sigma = data1['simple_return'].std()\n",
    "\n",
    "# Calculate the parametric VaR(95)\n",
    "z_score = norm.ppf(alpha) # this gives us the z-value at 5% (≈ -1.645)\n",
    "parametric_var = mu + z_score * sigma\n",
    "print(f\"Parametric VaR(95% Confidence): {parametric_var:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 95% confidence, we do not expect to lose more than 5.28% in a single day under the assumption of normally distributed returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuPl2RhmlBd9"
   },
   "source": [
    "**HINT**:  You can compute parametric VaR(90) using norm.ppf(confidence_level=0.10, mu, vol)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I18H5FAWOVuA"
   },
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZ2yuDGJmyg8"
   },
   "source": [
    "### Section 5: Scaling the VaR\n",
    "\n",
    "The VaR calculated in the previous sections is simply the value at risk for a single day. To estimate the VaR for a longer time duration, scale the value by the square root of time, similar to scaling volatility.\n",
    "\n",
    "The formula for this is:\n",
    "\n",
    " $\\text{VaR}_{\\text{t days}} = \\text{VaR}_{\\text{1 day}} * \\sqrt{t}$\n",
    "\n",
    "Based on the square-root-of-time rule, which is valid under the assumption that:\n",
    "\n",
    "**Returns are independent and identically distributed (i.i.d.) & No compounding is considered**\n",
    "\n",
    " Using the above formula, let us see how VaR increases over the time for a period of an year.\n",
    "\n",
    " Follow these steps:\n",
    "\n",
    " 1. Create an empty 2-d array of shape 252x2.\n",
    " 2. In a for loop, iterate through all the values of days (1-252) and add the time to the first column of the array.\n",
    " 3. Add the value of VaR for that time period to the second column of the array.\n",
    " 4. Plot the results by passing the array to the function plot_var() defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMrrZFvAIJlz"
   },
   "outputs": [],
   "source": [
    "def plot_var(array):\n",
    "  d = pd.DataFrame(abs(array))\n",
    "  d[1].plot(xlabel='Time', ylabel='Forecasted VaR-95', title = \"Time scaled VaR\")\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rR5ZaOwFn3Eu"
   },
   "outputs": [],
   "source": [
    "# Create an empty array to contain the VaR values\n",
    "VaR_arr = np.empty([252, 2])\n",
    "\n",
    "# Loop through the time period\n",
    "for i in range(1,253):\n",
    "  VaR_arr[i-1,0] = i # Day Number\n",
    "  VaR_arr[i-1,1] = abs(parametric_var) * np.sqrt(i) #scaling the VaR value by the square root of time\n",
    "\n",
    "# Plot the results\n",
    "plot_var(VaR_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcWEH3VXqIoR"
   },
   "source": [
    "**HINT**: VaR(t)= _VaR computed above_ * np.sqrt(t)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chart shows how much risk builds up the longer you hold a position.\n",
    "The curve illustrates our forecasted loss at a 95% confidence level, increasing over time.\n",
    "\n",
    "While daily risk might seem manageable — say, 5% — that risk compounds over time.\n",
    "After a year, the total potential downside could reach around 79% due to volatility accumulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55I9_02FOZnh"
   },
   "source": [
    "### **Section 6: Monte Carlo simulations**\n",
    "\n",
    "Follow these steps:\n",
    "\n",
    "1. Set the seed for the random number generator so that our results are reproducible.\n",
    "2. Compute the log returns.\n",
    "3. Compute the mean, variance, and standard deviation of the log returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ul2CtRRxOVQh"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Add the code for section 6 here\n",
    "'''\n",
    "# 1. Set the random seed for reproducibility\n",
    "np.random.seed(42)  # For reproducibility\n",
    "#2. Compute log returns using the formula: log_return = log(current price / previous price)\n",
    "data1['log_return'] = np.log(data1['Close'] / data1['Close'].shift(1))\n",
    "data1.dropna(inplace=True)\n",
    "#3. Calculate the mean and standard deviation of the log returns\n",
    "mu = data1[\"log_return\"].mean()\n",
    "var = data1[\"log_return\"].var()\n",
    "sigma = data1[\"log_return\"].std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzIug60kj1Iz"
   },
   "source": [
    "4. Compute the drift.\n",
    "5. Initialize the following parameters for simulations\n",
    "- n_days: the number of days\n",
    "- n_sims: the number of simulations. Here, we will run 1000 simulations.\n",
    "6. Compute the daily returns using the formula below:\n",
    "\n",
    "\n",
    "${returns}_{daily} = {e^r}$\n",
    "\n",
    "where, ${r}={drift}+{stdev*z}$\n",
    "\n",
    "7. Compute the VaR(95) and cVaR(95).\n",
    "8. Plot the simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "py2w5uLdo_oT"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Add the code for steps 4-8 here\n",
    "'''\n",
    "#4. Compute Drift\n",
    "drift = mu - (0.5 * var)\n",
    "\n",
    "#5. Set the number of days to simulate\n",
    "n_days = 10\n",
    "n_sims = 1000  # Number of simulations\n",
    "\n",
    "# 6. Generate returns using formula: returns = exp(drift + sigma * random_normal)\n",
    "Z = np.random.normal(size = (n_days, n_sims))\n",
    "daily_returns = np.exp(drift + sigma * Z)\n",
    "\n",
    "#7. Simulate price paths\n",
    "S0 = data1['Close'].iloc[-1]  # Last known price\n",
    "price_paths = np.zeros_like(daily_returns)\n",
    "price_paths[0] = S0\n",
    "\n",
    "for t in range(1, n_days):\n",
    "    price_paths[t] = price_paths[t - 1] * daily_returns[t]\n",
    "\n",
    "#8. Compute VaR(95) and CVaR(95)\n",
    "final_prices = price_paths[-1]\n",
    "returns = (final_prices - S0) / S0\n",
    "var_95 = np.percentile(returns, 5)  # VaR at 95% confidence\n",
    "cvar_95 = returns[returns <= var_95].mean()  # CVaR at 95% confidence\n",
    "\n",
    "\n",
    "print(f\"Monte Carlo VaR(95): {var_95:.5f}\")\n",
    "print(f\"Monte Carlo CVaR(95): {cvar_95:.5f}\")\n",
    "\n",
    "# Plot the simulated price paths\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(price_paths[:, :100], alpha=0.2)\n",
    "\n",
    "plt.title(\"Monte Carlo Simulation of NVDA Prices (10 Days, 1000 Simulations)\")\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Simulated Price\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHohKvDazKwj"
   },
   "source": [
    "**HINT**:\n",
    "- random_rets = np.random.normal(mean, std, T). Append the simulations to a list.\n",
    "- var(90) = np.percentile(_simulations list_, 10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chart shows the result of running a Monte Carlo simulation on NVDA's stock price over the next 10 trading days.\n",
    "\n",
    "Each line represents a possible future path that the stock could take, based on:\n",
    "\n",
    "- the historical mean and volatility of log returns\n",
    "- and a random shock component, which reflects market unpredictability.\n",
    "\n",
    "We’ve run 1,000 simulations, and plotted a sample of 100 to avoid clutter.\n",
    "\n",
    "All paths begin from today’s actual closing price — roughly $138 — and diverge over time.\n",
    "\n",
    "This simulation assumes that:\n",
    "\n",
    "- daily returns are normally distributed and follow a Geometric Brownian Motion, which is standard in finance for modeling asset prices.\n",
    "\n",
    "From this simulation, we calculate:\n",
    "\n",
    "- Monte Carlo VaR(95%) = –13.4%\n",
    "\n",
    "- Monte Carlo CVaR(95%) = –17.6%\n",
    "\n",
    "This tells us that:\n",
    "\n",
    "**There's a 5% chance the stock could lose more than 13.4% over the next 10 days — and if that happens, the average loss would be around 17.6%.**\n",
    "\n",
    "This Monte Carlo approach offers a forward-looking, probability-based view of risk — ideal for stress testing and scenario planning.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3m5wqtSttqA"
   },
   "source": [
    "## Step 2\n",
    "\n",
    "### Simulate the prices using Monte Carlo simulations.\n",
    "\n",
    "1. Predict the prices. Set the value of the last adjusted close price as p0.\n",
    "2. Initialize a numpy array of zeroes similar to the daily_returns computed above. You can use the function `np.zeros_like()`.\n",
    "3. Set the initial value of the price as p0.\n",
    "4. In a for loop for the number of days, we will fill the prices array with the prices computed from the `daily_returns` simulated above.\n",
    "5. Convert the prices array to a dataframe.\n",
    "6. Plot the simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gLiUJBctwC3"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Add the code for step 2 here\n",
    "'''\n",
    "p0 = data1['Close'].iloc[-1]  # Last known price\n",
    "prices = np.zeros_like(daily_returns)\n",
    "#3. set initial price\n",
    "prices[0] = p0\n",
    "# 4 fills the price paths\n",
    "for t in range(1, n_days):\n",
    "    prices[t] = prices[t - 1] * daily_returns[t]\n",
    "\n",
    "#5. Convert prices to dataframe\n",
    "price_df = pd.DataFrame(prices)\n",
    "\n",
    "#6. Plot the simulated price paths\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(price_df.iloc[:, :100], alpha=0.2)  # plot first 100 simulations\n",
    "plt.title(\"Monte Carlo Simulated Price Paths (10 Days, 1000 Simulations)\")\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Simulated Price\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot displays a Monte Carlo simulation of NVDA’s price over the next 10 days, based on its historical mean return and volatility.\n",
    "\n",
    "Each path is generated using a geometric Brownian motion model, which assumes:\n",
    "\n",
    "- Log returns are normally distributed\n",
    "\n",
    "- Price evolves by compounding small random shocks each day\n",
    "\n",
    "From this simulation, we can estimate the probability distribution of future outcomes. This gives us a forward-looking view of risk, allowing us to compute metrics like:\n",
    "\n",
    "- VaR: What’s the worst loss we expect with 95% confidence?\n",
    "\n",
    "- CVaR: And how bad could it get beyond that?\n",
    "\n",
    "This simulation provides richer insight than historical or parametric VaR alone because it models a range of future outcomes, not just static thresholds."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EPAT Guided Mini Project 01_modified_VK.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
